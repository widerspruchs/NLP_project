{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import preprocessor as prep_t\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dropout, LeakyReLU, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mislabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mislabeled = df.groupby(['text']).nunique().sort_values(by='target', ascending=False)\n",
    "df_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target_relabeled'] = df['target'].copy() \n",
    "df.loc[df['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife', 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target_relabeled'] = 1\n",
    "df.loc[df['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target_relabeled'] = 1\n",
    "df.loc[df['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_relabeled'] = 1\n",
    "df.loc[df['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target_relabeled'] = 1\n",
    "df.loc[df['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_relabeled'] = 1\n",
    "df.loc[df['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_relabeled'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['target'])\n",
    "df = df.rename(columns={'target_relabeled':'target'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only useful location values (countries, cities)  using spaCy's pre-trained NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gpe(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'GPE':\n",
    "            return ent.text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df.location.notnull(), 'gpe_extracted'] = df.loc[df.location.notnull(), 'location'].map(extract_gpe)\n",
    "df.loc[df.location.isin(['USA', 'Wordwide']), 'gpe_extracted'] =  df.loc[df.location.isin(['USA', 'Wordwide']), 'location']\n",
    "df.loc[df.location =='M!$$!$$!PP!', 'location'] = \"MISSISIPPI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text by removing \\x89, removing urls, mention and emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep_t.set_options(prep_t.OPT.URL, prep_t.OPT.EMOJI, prep_t.OPT.MENTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text']=df['text'].apply(lambda x:re.sub(r\"(.*[a-zA-Z]?)\\x89[^\\W]*([a-zA-Z]?.*)\", r\"\\1, \\2\", x))\n",
    "df['cleaned_text'] = df['cleaned_text'].map(prep_t.clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace consecutive dots by \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "consequitivedots = re.compile(r'\\.{2,}')\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: consequitivedots.sub(' ... ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove no-ASCII charcters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    \"\"\"\n",
    "        Remove non-ASCII characters \n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: remove_non_ascii(x))\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: x.replace('#', ' ').replace('@', \" \"))\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x.split()))\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part aims to create some meta data varaible about tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create several variables:\n",
    "* Create a list of frequent keywords for true disaster tweets. Assign 1 if the keyword is in the list else 0\n",
    "* numbre words in the tweet\n",
    "* mean length of words\n",
    "* number of characters\n",
    "* punctuation count\n",
    "* hashtag count\n",
    "* number of urls\n",
    "* url length\n",
    "* length of cleaned text\n",
    "* Create a list of frequent keywords for true disaster tweets. Assign 1 if the keyword is in the list else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.groupby('keyword')['target'].mean().reset_index()\n",
    "tmp = tmp.sort_values('target', ascending = False)\n",
    "\n",
    "true_keywords = tmp[tmp.target > 0.5].keyword.values\n",
    "\n",
    "df['is_true_keyword'] =  df['keyword'].isin(true_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_count\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_word_length\n",
    "df['mean_word_length'] = df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of characters\n",
    "df['char_count'] = df['text'].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['punctuation_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "df['hashtag_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "df['mention_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "\n",
    "df['url_count'] = df['text'].apply(lambda x: len(prep_t.parse(x).urls) if np.all(pd.notnull(prep_t.parse(x).urls)) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_url(text):\n",
    "    parsed_tweet = prep_t.parse(text)\n",
    "    c= 0\n",
    "    if np.any(pd.notnull(parsed_tweet.urls)):\n",
    "        for i in range(len(parsed_tweet.urls)):\n",
    "            c = c+ len(parsed_tweet.urls[i].match)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_url'] = df['text'].apply(lambda x: length_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_cleaned_text'] = df['cleaned_text'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.groupby('gpe_extracted')['target'].mean().reset_index()\n",
    "\n",
    "tmp = tmp.sort_values('target', ascending = False)\n",
    "\n",
    "true_gpe_extracted = tmp[tmp.target > 0.5].gpe_extracted.values\n",
    "\n",
    "df['true_gpe_extracted'] = df['gpe_extracted'].isin(true_gpe_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT + META DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['target', 'is_true_keyword', 'word_count', 'mean_word_length', 'char_count',\n",
    "       'punctuation_count', 'hashtag_count', 'mention_count', 'url_count',\n",
    "       'len_url', 'cleaned_text', 'len_cleaned_text', 'true_gpe_extracted']].copy()\n",
    "df['is_true_keyword'] = df['is_true_keyword'].map(int) #transform from boolean to int\n",
    "df['true_gpe_extracted'] = df['true_gpe_extracted'].map(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_cols = ['is_true_keyword','word_count', 'mean_word_length', 'char_count',\n",
    "                  'punctuation_count', 'hashtag_count', 'mention_count', 'url_count',\n",
    "                  'len_url', 'len_cleaned_text', 'true_gpe_extracted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntargets = 2\n",
    "seq_max=84\n",
    "nb_meta = len(meta_data_cols)\n",
    "\n",
    "loss = \"binary_crossentropy\"\n",
    "activation = \"softmax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select subsamples here (the training on overall dataset was done on colab to boost the training speed with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[:300].copy()\n",
    "df_val = df[300:400].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train set (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns = ['target'])\n",
    "y = df_train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform target column to a matrix\n",
    "y_categorical = to_categorical(y)\n",
    "nb_labels = len(np.unique(y))\n",
    "\n",
    "# with BERT tokenizer's batch_encode_plus batch of both the sentences areencoded together and separated by [SEP] token.\n",
    "sequence = X[\"cleaned_text\"].values.tolist()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased') \n",
    "seqs = tokenizer.batch_encode_plus(sequence,\n",
    "                                   max_length=seq_max, \n",
    "                                   pad_to_max_length=True)\n",
    "\n",
    "# convert batch of encoded features to numpy array.\n",
    "X_seq, X_attention = (np.asarray(seqs[\"input_ids\"]), np.asarray(seqs[\"attention_mask\"]))\n",
    "# tuple of metadata dataframe and number of metadata\n",
    "X_meta, nb_meta_features = (X[meta_data_cols], len(meta_data_cols))\n",
    "\n",
    "# create X_input and y_input \n",
    "X_input = [X_seq, X_attention, X_meta]\n",
    "y_input = y_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = df_val.drop(columns=['target'])\n",
    "y_val = df_val['target'].values\n",
    "y_categorical_val = to_categorical(y_val)\n",
    "\n",
    "sequence_val = X_val[\"cleaned_text\"].values.tolist()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "seqs_val = tokenizer.batch_encode_plus(sequence_val,\n",
    "                                   max_length=seq_max, \n",
    "                                   pad_to_max_length=True)\n",
    "\n",
    "X_seq_val, X_attention_val = (np.asarray(seqs_val[\"input_ids\"]), np.asarray(seqs_val[\"attention_mask\"]))\n",
    "X_meta_val, nb_meta_features_val = (X_val[meta_data_cols], len(meta_data_cols))\n",
    "\n",
    "X_input_val = [X_seq_val, X_attention_val, X_meta_val]\n",
    "y_input_val = y_categorical_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Keras tensor for text and attention_mask\n",
    "text_input = Input(shape=(seq_max,), dtype=\"int32\")\n",
    "attention_input = Input(shape=(seq_max,), dtype=\"int32\")\n",
    "\n",
    "# Create BERT architecture\n",
    "x = TFBertModel.from_pretrained(\"bert-base-cased\")(inputs = text_input, attention_mask = attention_input)[0][:, 0, :]\n",
    "\n",
    "# instantiate a Keras tensor for metadata\n",
    "Meta_input = Input(shape=(nb_meta,), dtype=\"float32\")\n",
    "\n",
    "# list of inputs\n",
    "inputs = [text_input, attention_input, Meta_input]\n",
    "\n",
    "# first NN model for metadata\n",
    "concatenate_1 = Meta_input\n",
    "\n",
    "y = Dense(150, activation=\"linear\")(concatenate_1)\n",
    "y = Dropout(0.2)(y)\n",
    "y = LeakyReLU(alpha=0.05)(y)\n",
    "y = Dense(100, activation=\"linear\")(y)\n",
    "y = Dropout(0.2)(y)\n",
    "y = LeakyReLU(alpha=0.05)(y)\n",
    "y = Dense(80, activation=\"linear\")(y)\n",
    "y = Dropout(0.2)(y)\n",
    "y = LeakyReLU(alpha=0.05)(y)\n",
    "\n",
    "# concatenate meta data embeddings with encoded text by BERT\n",
    "concatenate_2 = Concatenate(axis=1)([x, y])\n",
    "\n",
    "# NN for output\n",
    "z = Dense(200, activation=\"linear\")(concatenate_2)\n",
    "z = Dropout(0.2)(z)\n",
    "z = LeakyReLU(alpha=0.05)(z)\n",
    "z = Dense(100, activation=\"linear\")(z)\n",
    "z = Dropout(0.2)(z)\n",
    "z = LeakyReLU(alpha=0.05)(z)\n",
    "output = Dense(ntargets, activation=activation)(z)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=5e-5), loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit & estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_memory():\n",
    "    return psutil.virtual_memory()._asdict()['available']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "19/19 [==============================] - 125s 7s/step - loss: 1.5419 - accuracy: 0.5100 - val_loss: 0.6235 - val_accuracy: 0.7300\n",
      "Our model is using -376.1 Mb memory (RAM).\n"
     ]
    }
   ],
   "source": [
    "memory_start = get_available_memory()\n",
    "training_start = time.time()\n",
    "model.fit(X_input, y_input, batch_size=16, epochs = 1, validation_data=(X_input_val, y_input_val))\n",
    "training_end = time.time()\n",
    "model_memory = memory_start - get_available_memory()\n",
    "model_memory = round(model_memory / 1e9 * 1024 , 1)\n",
    "print('Our model is using {} Mb memory (RAM).'.format(str(model_memory)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit all train.csv data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See colab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.loc[df_test.location.notnull(), 'gpe_extracted'] = df_test.loc[df_test.location.notnull(), 'location'].map(extract_gpe)\n",
    "df_test.loc[df_test.location.isin(['USA', 'Wordwide']), 'gpe_extracted'] =  df_test.loc[df_test.location.isin(['USA', 'Wordwide']), 'location']\n",
    "df_test.loc[df_test.location =='M!$$!$$!PP!', 'location'] = \"MISSISIPPI\"\n",
    "\n",
    "df_test['cleaned_text']=df_test['text'].apply(lambda x:re.sub(r\"(.*[a-zA-Z]?)\\x89[^\\W]*([a-zA-Z]?.*)\", r\"\\1, \\2\", x))\n",
    "df_test['cleaned_text'] = df_test['cleaned_text'].map(prep_t.clean)\n",
    "df_test['cleaned_text'] = df_test['cleaned_text'].apply(lambda x: consequitivedots.sub(' ... ', x))\n",
    "df_test['cleaned_text'] = df_test['cleaned_text'].apply(lambda x: remove_non_ascii(x))\n",
    "df_test['cleaned_text'] = df_test['cleaned_text'].apply(lambda x: x.replace('#', ' ').replace('@', \" \"))\n",
    "df_test['cleaned_text'] = df_test['cleaned_text'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "df_test['is_true_keyword'] =  df_test['keyword'].isin(true_keywords)\n",
    "\n",
    "df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "df_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "df_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "df_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "df_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "df_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "\n",
    "df_test['url_count'] = df_test['text'].apply(lambda x: len(prep_t.parse(x).urls) if np.all(pd.notnull(prep_t.parse(x).urls)) else 0)\n",
    "\n",
    "df_test['len_url'] = df_test['text'].apply(lambda x: length_url(x))\n",
    "\n",
    "df_test['len_cleaned_text'] = df_test['cleaned_text'].map(len)\n",
    "\n",
    "df_test['true_gpe_extracted'] = df_test['gpe_extracted'].isin(true_gpe_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[['is_true_keyword', 'word_count', 'mean_word_length', 'char_count',\n",
    "       'punctuation_count', 'hashtag_count', 'mention_count', 'url_count',\n",
    "       'len_url', 'cleaned_text', 'len_cleaned_text', 'true_gpe_extracted']].copy()\n",
    "\n",
    "df_test['is_true_keyword'] = df_test['is_true_keyword'].map(int)\n",
    "df_test['true_gpe_extracted'] = df_test['true_gpe_extracted'].map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res = []\n",
    "for i in range(df_test.shape[0]):\n",
    "    df_test_copy = deepcopy(df_test.loc[i:i, :])\n",
    "    \n",
    "    \n",
    "    sequence_test = df_test_copy[\"cleaned_text\"].values.tolist()\n",
    "    #tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    seqs_test = tokenizer.batch_encode_plus(sequence_test,\n",
    "                                       max_length=seq_max, \n",
    "                                       pad_to_max_length=True)\n",
    "   \n",
    "    X_seq_test, X_attention_test = (np.asarray(seqs_test[\"input_ids\"]), np.asarray(seqs_test[\"attention_mask\"]))\n",
    "    \n",
    "    X_meta_test, nb_meta_features_test = (df_test_copy[meta_data_cols], len(meta_data_cols))\n",
    "    \n",
    "    X_input_test = [X_seq_test, X_attention_test, X_meta_test]\n",
    "    \n",
    "    y_res.append(np.argmax(model.predict(X_input_test), axis =1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv('./../data/test.csv')\n",
    "\n",
    "output = output[['id']]\n",
    "\n",
    "output['target'] = y_res\n",
    "#output[['id', 'target']].to_csv('submissions_bert_meta.csv', sep=\",\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minuteme",
   "language": "python",
   "name": "minuteme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
