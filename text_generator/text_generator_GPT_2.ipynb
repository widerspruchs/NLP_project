{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generator_GPT-2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Text Generator with fine-tune GPT-2\n",
        "\n",
        "This notebook aims to fine-tune a GPT-2 to generate texts with the same style that it learned from the given data. To do this, we use google colab free GPU to accelerate the training process and we sue `gpt-2-simple`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxWNbSwip5Gh"
      },
      "source": [
        "**Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "outputId": "71a425d1-a59c-4b31-d928-8303f4da8059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "outputId": "51fdd30f-e6be-49ab-beda-7625d5de1cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Oct  6 09:26:10 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "We download the GPT-2 model first. \n",
        "We use the samllest size of GPT-2: `124M` (default): the \"small\" model, 500MB on disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "outputId": "45cfb505-334b-4b02-c54f-e0b2ca9346bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 278Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 117Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 368Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:06, 79.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 279Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 183Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 203Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "We mount a Google Drive in the VM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "outputId": "f9f52436-b77c-455b-af84-2f4a2afdb7d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "file_name = \"quotes_gpt.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS"
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "We create a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "outputId": "6eba865b-6268-45da-c25c-c810a27bf3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:26<00:00, 26.66s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 4979602 tokens\n",
            "Training...\n",
            "[10 | 28.51] loss=3.73 avg=3.73\n",
            "[20 | 50.25] loss=3.57 avg=3.65\n",
            "[30 | 72.40] loss=3.23 avg=3.51\n",
            "[40 | 94.94] loss=3.54 avg=3.51\n",
            "[50 | 118.12] loss=3.54 avg=3.52\n",
            "[60 | 141.16] loss=3.49 avg=3.51\n",
            "[70 | 163.90] loss=3.42 avg=3.50\n",
            "[80 | 186.83] loss=3.20 avg=3.46\n",
            "[90 | 209.87] loss=3.35 avg=3.45\n",
            "[100 | 232.72] loss=3.35 avg=3.44\n",
            "[110 | 255.58] loss=3.75 avg=3.47\n",
            "[120 | 278.54] loss=3.44 avg=3.47\n",
            "[130 | 301.47] loss=3.43 avg=3.46\n",
            "[140 | 324.30] loss=3.51 avg=3.47\n",
            "[150 | 347.19] loss=3.32 avg=3.46\n",
            "[160 | 370.15] loss=3.45 avg=3.46\n",
            "[170 | 392.99] loss=3.42 avg=3.45\n",
            "[180 | 415.77] loss=3.27 avg=3.44\n",
            "[190 | 438.68] loss=3.40 avg=3.44\n",
            "[200 | 461.64] loss=3.70 avg=3.45\n",
            "======== SAMPLE 1 ========\n",
            "'t and that's great. That's good because in a world where every second counts, it's all you can think about, just like a thousand years ago, that might make life exciting. But even if they didn't make life exciting, you wouldn't know it at that moment. You can't ever really know which side you're on. And that's why, if they can get through it, why wouldn't I? Why wouldn't I have to go back to the same place every day, all those years? And you don't get to go back to all of the same places and go home?\n",
            "\n",
            "I have a feeling that, although you might not have a great life, you would be much happier with it than you would be with any other kind of existence. I don't know why. I think people think that all those years of you just being there for him and letting him go, and all that's happened and all the happiness and the hope and the excitement and the excitement and everyone's got a chance to do something really important and beautiful and special, something that we couldn't do, wouldn't be the sort of thing that you are and you really couldn't even ask for anything in return. The whole time, he only has to let you know that he loves me. And that's always very, very important and very special and makes you believe, sometimes, in your heart it doesn't matter at all in the end.\n",
            "\n",
            "Don't ever go about life as if it weren't important and worthwhile. Find ways to never forget the very existence of what you are, what you were created for. You're so important and you're so valuable to everyone around you, that's how it's going to be.\n",
            "\n",
            "A great life is worth more than its beauty or its drama.\n",
            "\n",
            "I don?t think that you can have a nice life with a thousand little flowers. I mean, it's hard to think about it. And, that's a very good thing. I think that it would be very, very good if there wasn?t that much going on in these days?\n",
            "\n",
            "You want to tell me, man, I want to be the King, I have no one telling me what to do now.\n",
            "\n",
            "I don?t think it? It's like there's nothing left to say. And I know that that's why I love you, and I want to be like you, and that's so all right with me. And I want everything to be beautiful, just with the stars and everything, just like there's no end to being happy?\n",
            "\n",
            "Maybe it would have been a good idea to take on an entire life of your own, just so they could say: 'We're going to spend our Sundays together,' rather than just be 'every Sunday in my life.'\n",
            "\n",
            "I always feel like you would like an end to something, no matter the pain or the suffering. For if it wasn't for your pain, or because of the pain or the suffering, I wouldn?t really be here. But I feel like that would be too much. I'd just start laughing about it.\n",
            "\n",
            "You’ll see, it wouldn?t be so much trouble to have an end to your life, would it?t actually be even so much fun?\n",
            "\n",
            "Your life has a purpose, and its purpose is happiness. The purpose is a new life, and you’re moving back to it.\n",
            "\n",
            "What you do is a way to say to yourself,” he said, “I cannot believe that a simple thing like that, I’ve got something wrong.”\n",
            "\n",
            "All religions are like religions, they come from inside of an individual, but the truth of the truth is, they transcend every type of religion. Religion is the truth where each thing is, but the other is, and each of us, is a God in a variety, but the truth is, we all believe, that God exists.\n",
            "\n",
            "No man will go and change who he is so soon, with no fear of change, for it is not a moment that ever moves. But now, he will change, he will change who he is, and that is the very thing that is keeping him from what?\n",
            "\n",
            "It is a wonderful thing to know people who are living things which you would not wish to live in.\n",
            "\n",
            "So now, people think the most beautiful thing that can be, to be about a certain time.\n",
            "\n",
            "So I say: the real things are different than you, and the more your life is changed, but I will be a happier one.\n",
            "\n",
            "I love all religions and religions and they all have a purpose.\n",
            "\n",
            "I am so excited, so excited, that we are almost at the end of our books. So when we look at them and look at books, there’s nothing more to say. All the things that make us happy are in our books, but then,\n",
            "\n",
            "[210 | 496.10] loss=3.43 avg=3.45\n",
            "[220 | 519.04] loss=3.41 avg=3.45\n",
            "[230 | 542.04] loss=3.43 avg=3.45\n",
            "[240 | 564.94] loss=3.47 avg=3.45\n",
            "[250 | 587.82] loss=3.62 avg=3.46\n",
            "[260 | 610.81] loss=3.56 avg=3.46\n",
            "[270 | 633.77] loss=3.42 avg=3.46\n",
            "[280 | 656.61] loss=3.28 avg=3.45\n",
            "[290 | 679.51] loss=3.32 avg=3.45\n",
            "[300 | 702.53] loss=3.25 avg=3.44\n",
            "[310 | 725.46] loss=3.34 avg=3.44\n",
            "[320 | 748.32] loss=3.42 avg=3.44\n",
            "[330 | 771.29] loss=3.42 avg=3.44\n",
            "[340 | 794.31] loss=3.37 avg=3.43\n",
            "[350 | 817.22] loss=3.68 avg=3.44\n",
            "[360 | 840.11] loss=3.42 avg=3.44\n",
            "[370 | 863.10] loss=3.42 avg=3.44\n",
            "[380 | 886.07] loss=3.37 avg=3.44\n",
            "[390 | 908.94] loss=3.37 avg=3.44\n",
            "[400 | 931.91] loss=3.39 avg=3.43\n",
            "======== SAMPLE 1 ========\n",
            " Creation, the ability to see in the dark, and the truthfulness of it all—all the things they have failed to see, and they think they know but have no power to get the answers.\n",
            "\n",
            "I believe in miracles that happen, when the work takes place. I do not believe in god. All it takes is a person who's not alone.\n",
            "\n",
            "The problem with all the theories that use the word \"faith\" but do not use the word \"mystical,\" is that they usually try to explain the entire story, by using a formula rather than the rules of logical reasoning that people have been using all their lives. As such, you can get a pretty strong sense of faith when you look at your religion, but you would only have the idea of how good it makes you feel when you look at your beliefs.\n",
            "\n",
            "To me, this book is just a bunch of nonsense. You think that it is perfect for science-fiction or reality or religion, but you don't really care. It's only the third book, so when you read this book, you'll get the idea of how it's all supposed to go. (You see, I don't believe in any religion other than God—no, not God, but God, because I'm scared of that and I won't help you anymore.) I don't care. When the first book shows up in our lives, I want you to hold your breath so that it isn't something that you can deny or ignore, and it will get you a great deal of success.\n",
            "\n",
            "A lot of people claim that religion teaches them to be religious, but not really to be. It teaches them that the true religion is the opposite of the true religion.\n",
            "\n",
            "I am not going to deny the evidence that any modern civilization is a real world. I have always suspected that, once the world is made up of religions, we tend to believe in less and less religion. For the better part of the last hundred years, religious belief has been reduced to an insufficiency of faith.\n",
            "\n",
            "It seems that the scientific method's success is dependent upon its ability to predict. Scientists need to know the answer. That's an important distinction.\n",
            "\n",
            "This isn't faith, it's faith! (John Dee)\n",
            "\n",
            "I am convinced that we need to study God as a philosopher or as a theologian. I think he should be an artist. He should be a philosopher of ethics. His work will not be influenced but it will be influenced by philosophy. And all the time the world is busy debating whether God exists or exists only at certain stages. And when there is no room for philosophy, then he will never have room for theology, he will never have theology, or theology, I think, he will never have theology. I think what is best for us is our faith.\n",
            "\n",
            "Every religion is different, and even if a person's religion is the same, even if he does not have a religion, he will know that there is at least some connection there, whatever that that is.\n",
            "\n",
            "The great problem with faith and any other belief is it creates an illusion and is in many cases just a guess.\n",
            "\n",
            "I am not a theologian, but I know that faith is necessary to live, and it's that essential to what I think, and the only way to live, I believe, is by believing.\n",
            "\n",
            "Do we really believe the world is as it is? If it is not, then it is not. There are certain words to that which say of God on earth: God exists; God is God.\n",
            "\n",
            "It's a dangerous belief, which gives a reason for people to doubt, to be convinced it's just a belief. Believe it so.\n",
            "\n",
            "They say you are a god, but I don't get it. All you know is the one thing in this world that makes God and God's God each a different person, even if you never saw one.\n",
            "\n",
            "Do we have to know it was the devil that brought Jesus to Christ?\n",
            "\n",
            "This is the reason it's so important that people have doubts and I don't have any objections to it. And I haven't ever been offended with it in any of my life.\n",
            "\n",
            "I was not born this way. It is not God's own way, but God's own.\n",
            "\n",
            "When the people see the bad things that happen to others and their parents, they have one thing left to do. They have to go to the Bible.\n",
            "\n",
            "The problem with being a scientist does not necessarily lie in being a Christian. It's not in being a scientist...It's not in believing any more than being a scientist. It's more about whether or not you believe that what God has created is something that makes God the source of all creation. People are born to believe in God, but not to believe and therefore the things they believe are not true.\n",
            "\n",
            "The real life in the world, is not limited to any one position\n",
            "\n",
            "[410 | 965.51] loss=3.49 avg=3.44\n",
            "[420 | 988.39] loss=3.08 avg=3.43\n",
            "[430 | 1011.26] loss=3.43 avg=3.43\n",
            "[440 | 1034.12] loss=3.55 avg=3.43\n",
            "[450 | 1057.10] loss=3.55 avg=3.43\n",
            "[460 | 1080.07] loss=3.62 avg=3.44\n",
            "[470 | 1103.00] loss=3.31 avg=3.43\n",
            "[480 | 1125.88] loss=3.55 avg=3.44\n",
            "[490 | 1148.74] loss=3.50 avg=3.44\n",
            "[500 | 1171.64] loss=3.28 avg=3.43\n",
            "Saving checkpoint/run1/model-500\n",
            "[510 | 1197.36] loss=3.47 avg=3.44\n",
            "[520 | 1220.37] loss=3.41 avg=3.44\n",
            "[530 | 1243.29] loss=3.47 avg=3.44\n",
            "[540 | 1266.18] loss=3.26 avg=3.43\n",
            "[550 | 1289.13] loss=3.26 avg=3.43\n",
            "[560 | 1312.13] loss=3.31 avg=3.42\n",
            "[570 | 1335.12] loss=3.31 avg=3.42\n",
            "[580 | 1358.08] loss=3.29 avg=3.42\n",
            "[590 | 1381.05] loss=3.31 avg=3.42\n",
            "[600 | 1403.97] loss=3.38 avg=3.42\n",
            "======== SAMPLE 1 ========\n",
            "�I’d never been on my mind that muchMore love, my brotherI had a dream of you and then there I found you and you were all I ever knewWho is he?Who has never been lonelyYou made me feel I’ve known this.And then I had a dream about you and you were all I ever knewThe only man I ever knewWho is now deadWhen we were alive,Who knows, when we diedI thought I was a boyI couldn’t have been on my mindBut stillI’ve always known you and he and you were my little manAnd then I had a dream about you and you were all I ever knewThe only man I ever knewWho was now deadWhen we were alive,Who knows, when we diedI thought I was a boyI couldn’t have been on my mindBut stillI’ve always known you and you were my little manAnd then I had a dream about you and he was my little manand then I had a dream about you and I was the man I knowWe died together.Who is he, Who is I who I amand now I am the man I knowWho is now deadWhen we were alive,Who is I who I amand now I am the man I knowWho will be my loveIf you stay together with me, I swearI am my love\n",
            "\n",
            "It is a great tragedy when they are all gone.\n",
            "\n",
            "I was on good terms with you for a long time, because before I made you my slave, I wanted you to be with me.\n",
            "\n",
            "There is only one good God, who knows how to get us right. He will make us a lot better than we are now.\n",
            "\n",
            "You need to get off that mountain cliff, and off that rock cliff, and on that rock cliff. You need love everywhere.\n",
            "\n",
            "How long will it take you to get used to it? I would have a good job, I am just too happy to let the wind of this world give it away. When I was little I thought I was too happy. Now I'm used to it now.\n",
            "\n",
            "They say it's better to love yourself for the first time than it is to love someone for the second time. That is why when you lose a friend, you lose that person, and then it's like, oh wow. They never give a fuck. When someone falls, they want their soulmate back, not the body they were with.\n",
            "\n",
            "A word, a word, a word, a word, and one word, a word, I have spent the rest of my life with you.\n",
            "\n",
            "When they were still alive, I used to talk of how much they liked you. I used to think that's as great as when it was your sister, or your baby. It’s not. You don’t have to think that way. I used to think that was ridiculous. It’s like the sound of a baby being born.\n",
            "\n",
            "Why does my heart ache so often when I see you?” He held her hand up.I held it out cold, to make sure I had done my part. He shook his head, looking back at me with an annoyed expression. He shrugged. I don’t know why, but something about that was enough to make me fall asleep like that.\n",
            "\n",
            "I cannot speak to our family's happiness from its tragic history. We always think that we never know a person's happiness, the only truth we can use is that our family.\n",
            "\n",
            "When you know the right person, you know who to call. The key to love is not to be left out on the cliff.\n",
            "\n",
            "If you think you could hurt this man and it won't work out, let him go. If it does he will say anything and everything but his words will not change. The man is already there and if you don't tell him, he will be out a fool and you can't trust him. And if you don't give out your word, or you're the one who doesn't follow your own advice, then it won't be true, and so he will say nothing. Now I can trust who you trust. I cannot tell who he is but I can tell how you trust me, or trust me but I can't do both.\n",
            "\n",
            "Don’t hold on to that man and let him go. Hold your hand and let them go.\n",
            "\n",
            "I am your love when the pain makes me feel strong.\n",
            "\n",
            "I think we've crossed a line. In my opinion, humans tend to die a lot faster with those who are around us, because they never need to die to get back on their feet. Our souls are made for death, so they are more likely to survive a lot.\n",
            "\n",
            "My body belongs to you, but my soul belongs to the God I have made myself into.\n",
            "\n",
            "People like to live, but the rest\n",
            "\n",
            "[610 | 1437.54] loss=3.55 avg=3.42\n",
            "[620 | 1460.45] loss=3.24 avg=3.42\n",
            "[630 | 1483.37] loss=3.41 avg=3.41\n",
            "[640 | 1506.32] loss=3.21 avg=3.41\n",
            "[650 | 1529.26] loss=3.46 avg=3.41\n",
            "[660 | 1552.24] loss=3.45 avg=3.41\n",
            "[670 | 1575.17] loss=3.37 avg=3.41\n",
            "[680 | 1598.10] loss=3.31 avg=3.41\n",
            "[690 | 1621.01] loss=3.30 avg=3.41\n",
            "[700 | 1643.92] loss=3.31 avg=3.41\n",
            "[710 | 1666.80] loss=3.46 avg=3.41\n",
            "[720 | 1689.75] loss=3.40 avg=3.41\n",
            "[730 | 1712.66] loss=3.20 avg=3.40\n",
            "[740 | 1735.56] loss=3.28 avg=3.40\n",
            "[750 | 1758.54] loss=3.34 avg=3.40\n",
            "[760 | 1781.49] loss=3.16 avg=3.39\n",
            "[770 | 1804.34] loss=3.31 avg=3.39\n",
            "[780 | 1827.31] loss=3.33 avg=3.39\n",
            "[790 | 1850.32] loss=3.24 avg=3.39\n",
            "[800 | 1873.19] loss=3.40 avg=3.39\n",
            "======== SAMPLE 1 ========\n",
            " avoid you; and that's okay. I was a little nervous, and I like the feeling I got from knowing that I was in love with someone even earlier.\n",
            "\n",
            "The heart of the mind dwells in love\n",
            "\n",
            "Be a man of love - and that is what men are, after all.\n",
            "\n",
            "I need nothing more than to give myself back to men, for a man can give himself so freely. For this I want to take from them.\n",
            "\n",
            "We are a race to the last, for we have to live life, not see.\n",
            "\n",
            "We are all in one and it is in this, not what others wish for, for each of us must be a part of the same, shared, self-governing society we live in.\n",
            "\n",
            "I am not a great love-fantasy writer.\n",
            "\n",
            "When we're alone, and with no one, and no place, when we're in other people's lives, it can be lonely.\n",
            "\n",
            "Love is the power of a woman. She can turn us into that which we are, a woman capable of expressing love in all kinds of ways, as well as from any kind. It is this kind of love I am alluding to. When love is revealed to all, it gives a voice of hope.\n",
            "\n",
            "A man should have no choice but to be loved by his wife, but if he loves his wife by her, then the duty of love may be reversed.\n",
            "\n",
            "The great poet of our generation, H.L. Mencken, has said that love is always strong, but when love is weak, a man is more likely to hate himself for not living with love. Love was once, he said, a man's own folly to hate. In love, all men are fools.\n",
            "\n",
            "I would like to say to you: “As my father said, your heart's a bit bitter and I like being lonely; but if I do everything I can’t get out of it, then my heart'll be broken again.\n",
            "\n",
            "You must not look down and judge. You must look upward and judge.\n",
            "\n",
            "I'm afraid to say it. I don't love people who are like it. People who try to please you with their clothes and their mannerisms.\n",
            "\n",
            "Love is like a knife to the heart, it cuts not on sight, not on principle.\n",
            "\n",
            "We are the first people we trust or fear, the last. The first people to realize that if they don't love each other, then God's judgement on them runs away from them to the far corners of the earth.\n",
            "\n",
            "The only real love that you will ever truly have is that one of you.\n",
            "\n",
            "A loving neighbor is one who listens to your heart and listens to your soul.\n",
            "\n",
            "Let us be strong. Let us love that which we should not be afraid of because we know that in love we are a part of another.\n",
            "\n",
            "That, in my opinion, is the very most vital love in the world.\n",
            "\n",
            "Don't give your heart away only for the love of another. Don't sell your soul for the love of Christ. He is the greatest enemy of the soul.\n",
            "\n",
            "Every day, we have to ask ourselves: How many days did you forget? Would you make it out with the grace of Jesus Christ? Would you make it out with peace and the power of God?\n",
            "\n",
            "How can one forget, when one would otherwise die, the words of a saint?\n",
            "\n",
            "If you've got the love of God in you, you don't have to love everybody.\n",
            "\n",
            "To love a girl is too hard, it makes it hard for her to forgive;to love her so long, that she will lose her heart.\n",
            "\n",
            "But this man’s mind was always too strong for her. It would break her heart all her she had ever known. And the best thing could happen, would be that she would know and see love come to her at all.\n",
            "\n",
            "They said that it was the beauty of your soul that made love last, but that you might be wrong.\n",
            "\n",
            "The only way things can change is to live with them.\n",
            "\n",
            "When you’re sad, you always cry. If you really love someone, you’re not the thing you should look for\n",
            "\n",
            "Some days you can't help but feel like someone you're not. Some days you realize you are. In the middle of the night, you realize you are. Just at the moment, those moments are the last people you would go to.\n",
            "\n",
            "Love is a long lost love. It has endured forever.\n",
            "\n",
            "There is nothing like loving a person to love you more.\n",
            "\n",
            "I don't love him very much,\" I said, \"and he doesn't even pretend to hold me back. It’s like he tries to be a father figure. But there's the truth, and that’s what I like. He doesn't\n",
            "\n",
            "[810 | 1906.49] loss=3.43 avg=3.39\n",
            "[820 | 1929.44] loss=3.21 avg=3.39\n",
            "[830 | 1952.29] loss=3.23 avg=3.38\n",
            "[840 | 1975.23] loss=3.19 avg=3.38\n",
            "[850 | 1998.24] loss=3.56 avg=3.38\n",
            "[860 | 2021.10] loss=3.35 avg=3.38\n",
            "[870 | 2043.99] loss=3.49 avg=3.39\n",
            "[880 | 2066.99] loss=3.17 avg=3.38\n",
            "[890 | 2089.94] loss=3.22 avg=3.38\n",
            "[900 | 2112.80] loss=3.46 avg=3.38\n",
            "[910 | 2135.81] loss=3.24 avg=3.38\n",
            "[920 | 2158.81] loss=3.58 avg=3.38\n",
            "[930 | 2181.69] loss=3.22 avg=3.38\n",
            "[940 | 2204.62] loss=3.34 avg=3.38\n",
            "[950 | 2227.64] loss=3.20 avg=3.38\n",
            "[960 | 2250.57] loss=3.42 avg=3.38\n",
            "[970 | 2273.42] loss=3.07 avg=3.37\n",
            "[980 | 2296.39] loss=3.13 avg=3.37\n",
            "[990 | 2319.35] loss=3.23 avg=3.36\n",
            "[1000 | 2342.23] loss=3.26 avg=3.36\n",
            "Saving checkpoint/run1/model-1000\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K"
      },
      "source": [
        "We copy the checkpoint folder to your own Google Drive to not loose the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "**Load**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "#gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "outputId": "118ce36d-6ac0-41a4-e8f8-0baf8eae2efd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#sess = gpt2.start_tf_sess()\n",
        "#gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run1/model-1000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "We generate text with fine-tuned model. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "outputId": "389a94d5-5e26-4dde-b2db-c57bd815c334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are two great traits of a teacher: (1) a passion for teaching, and (2) a passion for teaching.\n",
            "\n",
            "For most of the history of our species, there have been two kinds of teachers: those who are able to give us a good information about the world, and those who are able to put words in our brains. We might like to think that all kinds of teachers can do this, but it's hard to believe that anyone can do that.\n",
            "\n",
            "The stage of teaching is always changing and the end of the stage is always changing.\n",
            "\n",
            "I used to think that the best way to learn is by doing. But I didn't know that knowledge was an act: it takes a person to do something about the world, and the world doesn’t know what the hell is going on, so it leaves things to chance.\n",
            "\n",
            "The meanings of words are the same as the meanings of words: they are the same as the meanings of words.\n",
            "\n",
            "There has been a great deal of self-education since the time of the poet's father, and I have often endeavoured to find out why this was so often the case. The dogmatic human mind is always facing the question of what it ought to do, and if it wishes to do it, it has to do it. This, it is said, is the reason why the world can be taught, and why it is so hard to learn.\n",
            "\n",
            "Teachers don't require any sort of motive to teach. They are taught by the Master of the whole world, and by the teacher of the world.\n",
            "\n",
            "When a person finishes his or her education and is satisfied that his or her education is good and is complete, he or she is entitled to continue his or her education, but he or she may not continue it till he or she has finished it, until the end of his or her career.\n",
            "\n",
            "Teaching is the process of learning a new language and a new language of ideas, that is, the process of creating a new language.\n",
            "\n",
            "Teachers are not a group of people who have a great deal of money, who have a lot of money, and who all have a certain amount of money. That's what they do. They're not a group of people who have done a lot of bad work, people who have done a lot of bad work.\n",
            "\n",
            "A good teacher is somebody who is very good at teaching and who knows the students well at the subject matter and who knows what to do with the students.\n",
            "\n",
            "If you want to make a living by teaching, you have to do it right.\n",
            "\n",
            "If I could keep my head up, I'd be able to get up at any moment. But I don't have a head up. I'm doing it for myself, in a way or another. I'm not a writer.\n",
            "\n",
            "There are so many men who have mastered the art of teaching, and yet they will not use it to their advantage, to get to the truth of the matter.\n",
            "\n",
            "A good teacher is someone who is very good at teaching and who knows the students well at the subject matter and who knows what to do with the students.\n",
            "\n",
            "Teaching is a way of being a teacher, a way of being a teacher, a way of being a teacher, and a way of being a teacher.\n",
            "\n",
            "I used to love to read, to listen to music, to read, to talk. But now I've run out of those things. I've almost forgotten why they've been there: because they're there. They've been there for me. I've been in a lot of places I've never been. I've been in a lot of places I've never known. I've been in the middle of the streets, where the people are so busy being happy and happy. But at some point, when I've been in those places, I've become very lonely, and when I'm in those places, I've felt very lonely, almost.\n",
            "\n",
            "When you've got a teacher, you get a good teacher, but you don't get a good teacher.\n",
            "\n",
            "To be a teacher is the most important thing you can do. And it's not just the teacher who gets to do it, but the students as well.\n",
            "\n",
            "Teachers are the ones who make the most money.\n",
            "\n",
            "Teachers are people who make the most money.\n",
            "\n",
            "A good teacher is someone who is very good at teaching.\n",
            "\n",
            "A good teacher is someone who is very good at teaching.\n",
            "\n",
            "Teachers are people who make the most money.\n",
            "\n",
            "Teachers are people who make most money.\n",
            "\n",
            "Teachers are people who make the most money.\n",
            "\n",
            "A good teacher is someone who is very good at teaching.\n",
            "\n",
            "Teachers are people who make the most money.\n",
            "\n",
            "A good teacher is someone who is very good at teaching.\n",
            "\n",
            "Teachers are people who make the most money.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "We generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, we can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate`:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "outputId": "8c9c22fa-2990-4a90-8a56-e6a92e60389a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=\"Life\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Life.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "Love is a language of infinite simplicity\n",
            "\n",
            "Love is the language of endless complexity\n",
            "\n",
            "Love is the language of impossible complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of endless complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinity complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "Love is the language of infinite complexity\n",
            "\n",
            "====================\n",
            "Life. There are some who would choose to die but not all. There are some who would choose to leave but not all. There are some who would choose not to be born but not all.\n",
            "\n",
            "The only problem is that we are in the wrong body.\n",
            "\n",
            "But still my desire to be a crazy girl and a prom queen was stronger than any of the things that would make me crazy, because it was more painful than anything.\n",
            "\n",
            "There is no such thing as a single perfect woman. There is only a single Man.\n",
            "\n",
            "If a man was to do the thing he does, he would do it anyway.\n",
            "\n",
            "Olivia, if you could not say the things you love about yourself, you'd never know what to do with them.\n",
            "\n",
            "In case you thought I was a little misleading, I’ve been going to church every Sunday since the 1970s. And I’ve been really busy lately. I’ve been away from my kids, away from my wife, away from the family. I’ve been out of school, though. I’ve been away from the family, out of church, out of the morning. I’ve been at a\n",
            "====================\n",
            "Life, but instead of thinking of one's own life to be kept from it, that life is to be saved from the centrifugal forces of the world, for it is the same thing with your own life; and by knowing that your failure to find your own means of survival will never be your undoing, you can make a change, you can start a new life, and so on, until you find the strength to live your own way.\n",
            "\n",
            "The great idea of the Renaissance was that all things should be done in art, not by a man, not by one, but by all. The great idea of the Renaissance, however, was that we should not be forced to do something, by a choice, but by a debt. The great idea of the Renaissance was that we should not be forced to do something, by a choice, but by a debt. The great idea of the Renaissance was that all things should be done in art, not by a man, not by one, but by all. The great idea of the Renaissance was that all things should be done in art, not by a man, not by one, but by all. The great idea of the Renaissance was that we should not be forced to do something\n",
            "====================\n",
            "Life.\n",
            "\n",
            "I have a lot of friends who read all the great books I have read, but I don't have a great way of telling them they are wrong.\n",
            "\n",
            "I am going to have a very hard time getting out of bed on time, that's all. If I do that I am going to have a hard time staying alive.\n",
            "\n",
            "I didn't think so. I thought that this was a great place to live. I didn't think so. I thought that my parents were there.\n",
            "\n",
            "A great fear is a fear that something new will happen.\n",
            "\n",
            "I'm afraid of what I don't know, because I don't know what I don't know.\n",
            "\n",
            "I have a habit of using words for things that are not true.\n",
            "\n",
            "How often have we somehow begun to understand our own thoughts, and the ones that make us happy and, in a sense, the ones that make us more confident?\n",
            "\n",
            "The most important thing a person can do to your life is to keep your thoughts and actions hidden. Keep them alive and sincere.\n",
            "\n",
            "The joy of a thought is the experience of living and being alive. Life is full of trials, tribulations, sorrows and disappoint\n",
            "====================\n",
            "Life.\n",
            "\n",
            "It is a sacred duty of the heart to give rest to soul and body and soul to mind and spirit to heart.\n",
            "\n",
            "You can neither trust nor trust\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "The essence of good is trust\n",
            "\n",
            "You cannot trust your neighbour, even if you can trust your neighbour.\n",
            "\n",
            "You cannot trust your neighbour\n",
            "\n",
            "You cannot trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust your neighbour\n",
            "\n",
            "You cannot trust nor trust your neighbour\n",
            "\n",
            "You can neither trust nor trust\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07aI7uo81J3m"
      },
      "source": [
        "The results look good gramatically. The sentences make sense and look alike quotes. However, it repeats sometimes the same sentence. This phenomene is due to \"tempearture\" parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, we can generate a large amount of text to a file and sort out the samples locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g"
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "Thanks to [Max Woolf](http://minimaxir.com) for the notebook work to share how to fine-tune easily a GPT-2 with colab."
      ]
    }
  ]
}